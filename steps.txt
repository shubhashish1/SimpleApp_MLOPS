1. Create an environment named wineq and acitvate that conda environment.
2. Create requirements.txt file with following requirements listed:
     .dvc
     .dvc[gdrive]
     .scikit-learn
     . pandas
    Now install the requirements by command pip install -r requirements.txt
3. Now let's create a README.md file and put all steps in that.
4. Now let's create a template file to create the project folder structure with name template.py
5. Put the templates code which are nothing but the dirs and files we want to create and run by command : python template.py
6. Create a folder named data_given where we will keep our data and also it can be used as our remote data repo.
7. put the winequality.csv file there.
8. Now initialize git and dvc and also push the data to dvc:
     .git init
     .dvc init
     .dvc add data_given/winequality.csv
     .git add .
     .git commit -m "first commit"
     .git branch -M main
     .git remote add origin https://github.com/shubhashish1/SimpleApp_MLOPS.git
     .git push -u origin main

     â€¦or push an existing repository from the command line
git remote add origin https://github.com/shubhashish1/SimpleApp_MLOPS.git
git branch -M main
git push -u origin main

9. Then we have to put all the necessary params in the params.yaml
10. Now let's create our first file in src to get the data in the name get_data.py
11. Read the params.yaml file via this file and try to fetch the data source dynamically and create a dataframe.
12. Now let's create a file to load that data from get_data.py put it in the data/raw through file load_data.py
13. Create a method to store the raw data in the raw folder in the load_data.py file.
14. Now we have to specify all the stages that are needed to be performed for the project to run in the dvc.yaml
15. Let's update this load_data to raw folder as first stage in dvc.yaml
        stages:
            load_data:
                cmd: python src/load_data.py --config=params.yaml
                deps:
                -src/get_data.py
                -src/load_data.py
                -data_given/winequality.csv
                outs:
                -data/raw/winequality.csv
16. Now we can run the dvc.yaml by command dvc repro and this will create a dvc.lock file.
17. This dvc.lock file keeps the track of every changes done in individual stages.
18. Create a method to store the raw data in the raw folder in the split_data.py file.
19. Now we have to specify all the stages that are needed to be performed for the project to run in the dvc.yaml
20. Let's update this load_data to raw folder as first stage in dvc.yaml
        split_data:
            cmd: python src/split_data.py --config=params.yaml
            deps:
            - src/split_data.py
            - data/raw/winequality.csv
            outs:
            - data/processed/train_winequality.csv
            - data/processed/test_winequality.csv
21. Now we can run the dvc.yaml by command dvc repro and this will create a dvc.lock file.
22. This dvc.lock file keeps the track of every changes done in individual stages as it runs and skips the load_data stage and then runs the split_data stage.
23. Create a method to store the raw data in the raw folder in the train_and_evaluate.py file.
19. Now we have to specify all the stages that are needed to be performed for the project to run in the dvc.yaml
20. Let's update this load_data to raw folder as first stage in dvc.yaml
        train_and_evaluate:
            cmd: python src/train_and_evaluate.py --config=params.yaml
            deps:
            - data/processed/train_winequality.csv
            - data/processed/test_winequality.csv 
            - src/train_and_evaluate.py
            params:
            - estimators.ElasticNet.params.alpha
            - estimators.ElasticNet.params.l1_ratio
            metrics:
            - report/scores.json:
                cache: false
            - report/params.json:
                cache: false
            outs:
            - saved_models/model.joblib
21. Now we can run the dvc.yaml by command dvc repro and this will create a dvc.lock file.
22. This dvc.lock file keeps the track of every changes done in individual stages as it runs and skips the load_data,split_data stage and then runs the train_and_evaluate stage.
23. Here we are also making params.json and socres.json in report folder to track the params and metrics changes.
24. We can check the dvc metrics or scores by command: dvc metrics show

o/p: 

Path                alpha    l1_ratio    mae      r2       rmse
report\scores.json  -        -           0.65515  0.01301  0.80312
report\params.json  0.9      0.4         -        -        -


25. We can track the difference of metrics between old and new if we have retrained the model with modification
    with command: dvc metrics diff
26. Now we can doa testing by changing the alpha and l1_ratio in the params.yaml
27. Then let's run following commands:
       dvc repro
       dvc metrics diff

    This is the output we have received: (alpha changed from 0.9 to 0.88 and l1_ratio from 0.4 to 0.89)
      Path                Metric    HEAD     workspace    Change
report\scores.json  mae       0.65515  0.65982      0.00467
report\scores.json  r2        0.01301  0.00838      -0.00463
report\scores.json  rmse      0.80312  0.805        0.00188
report\params.json  alpha     0.9      0.88         -0.02
report\params.json  l1_ratio  0.4      0.89         0.49
28. To check or rollback the previous changes in git go to the commits and then copy the repo of any commit you want.
29. Now let's write codes for testing. For that we will install pytest and tox libaries via requirements.txt

TOX:

It is an automation testing package which has following params:
    . It will create a env based on our requirement for testing which can be one env or multiple envs.
      [tox]
          envlist=py27,py36

    . [testenv]
          deps=pytest
    . commands= ....

    To make tox work we have to create a tox.ini file in the root directory with following details:
        [tox]
        envlist= py37
        skipsdist= True

        [testenv]
        dep= -rrequirements.txt
        commands=
        pytest -v
    
    Now we have to create a directory to put our all testcases that we will write for testing.
    In that directory we will have 2 files named __init__.py, conftest.py and test_config.py

    Now if we wnat to create any method in these files for testing then we have to specify with test_(functionname)
    for the pytest to identify it. Then we can put Assert keyword to compare actual and predicted. If it satisfies
    then testcase passed or Failed. We can run it using command pytest -v or tox

    . If we want tox to recreate the env it has already created as it will always use the env in has created already
      then use command: tox -r
    . So tox basically runs whatever we have specified in the tox.ini file.

30. Let's create a new file named setup.py which will help us to make our codes convert into packages in the folders where we have __init__.py present.
31. Enter all the necessary details in the setup.py and then create the package with command pip install -e .
32. Now once the package is created and installed we can check it byt command pip freeze
33. Now let's create a wheel for this package so that we can use and install it as a package we use like numpy, pandas etc.
    with command python setup.py sdist bdist_wheel (sdist is std distribution and bdist_wheel for building wheel for installation)
34. Now let's comment the skipsdist= True in the tox.ini file.
35. We will check for the python syntax and coding standards using flake8 package of python:
         . pip install flake8
         . put below details in tox.ini for the testing:
                # stop the build if there are Python syntax errors or undefined names
                flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
                # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
                flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
36. Let's rebuild the tox env with command tox -r
37. Now let's create few folders prediction_service, prediction_service/model and webapp
38. Let's also create files as app.py and prediction_service/__init__.py and prediction_service/prediction.py
39. Let's create static folder inside webapp as we will be using flask.
40. Inside static we will be creating css and script.
41. Inside css folder let's create main.css file and inside script folder let's create index.js file.
42. Now let's create webapp/templates folder.
43. Now let's create html files inside templates folder as index.html, 404.html for errors, base.html.
44. Enter the details in respective html templates.
45. Now open app.py and put the app details for UI as well as the API testing.